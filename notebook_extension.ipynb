{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T20:53:54.009334Z",
     "iopub.status.busy": "2025-09-03T20:53:54.008995Z",
     "iopub.status.idle": "2025-09-03T20:54:08.515499Z",
     "shell.execute_reply": "2025-09-03T20:54:08.514708Z",
     "shell.execute_reply.started": "2025-09-03T20:53:54.009302Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# GPU: CUDA 11.8, PyTorch 2.0.1 su Kaggle\n",
    "!pip install --upgrade pip\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "!pip uninstall -y kaolin\n",
    "\n",
    "\n",
    "!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 \\\n",
    "  -f https://download.pytorch.org/whl/cu118/torch_stable.html\n",
    "\n",
    "\n",
    "!pip install kaolin==0.17.0 \\\n",
    "  -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.0.1_cu118.html\n",
    "\n",
    "!pip install tqdm pillow\n",
    "!rm -rf /kaggle/working/Affordance_Highlighting_Project_2024\n",
    "!rm -rf /kaggle/working/output\n",
    "!git clone https://github.com/MirkoDiMa/Affordance_Highlighting_Project_2024.git\n",
    "%cd Affordance_Highlighting_Project_2024\n",
    "import sys\n",
    "\n",
    "sys.path.append('/kaggle/working/Affordance_Highlighting_Project_2024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T20:54:08.517698Z",
     "iopub.status.busy": "2025-09-03T20:54:08.517451Z",
     "iopub.status.idle": "2025-09-03T20:54:08.523771Z",
     "shell.execute_reply": "2025-09-03T20:54:08.523001Z",
     "shell.execute_reply.started": "2025-09-03T20:54:08.517670Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ─── EXPERIMENT CONFIG ──────────────────────────────────────────────────────────\n",
    "exp_config = {\n",
    "    # Prompt\n",
    "    \"prompt\": \"A 3D render of a gray candle with highlighted hat\",\n",
    "\n",
    "    # Seed \n",
    "    \"seed\": 45,\n",
    "\n",
    "    # Data & path\n",
    "    \"obj_path\":        \"data/candle.obj\",\n",
    "    \"output_dir\":      \"/kaggle/working/output\",\n",
    "\n",
    "    # CLIP\n",
    "    \"clip_model_name\": \"ViT-L/14\",\n",
    "\n",
    "    # MLP\n",
    "    \"mlp_input_dim\":   3,\n",
    "    \"mlp_hidden_dim\":  256,\n",
    "    \"mlp_num_layers\":  6,\n",
    "    \"mlp_out_dim\":     2,\n",
    "    \"positional_encoding\": False,\n",
    "    \"sigma\":           5.0,\n",
    "\n",
    "    # Training\n",
    "    \"render_res\":      224,\n",
    "    \"n_views\":         6,\n",
    "    \"learning_rate\":   1e-4,\n",
    "    \"n_iter\":          2500,\n",
    "    \"n_augs\":          5,\n",
    "    \"clipavg\":         \"view\",\n",
    "\n",
    "    # Augmentation\n",
    "    \"aug_type\":        \"RandomPerspective\",\n",
    "    \"aug_params\": {\n",
    "        \"distortion_scale\": 0.5,\n",
    "        \"p\":               0.8,\n",
    "    },\n",
    "    #extension\n",
    "    \"bg_mode\":           \"noise\",     # \"none\" | \"solid\" | \"noise\" | \"image\" | \"mixed\"\n",
    "    \"bg_prob\":           0.75,        # probability of applying a background per batch of views\n",
    "    \"bg_key_color\":      [1.0, 1.0, 1.0],  # colour of rendering used as \"green screen\"\n",
    "    \"bg_key_tol\":        0.02,        # key color matching tolerance (0..1)\n",
    "    \"bg_dir\":            \"data/backgrounds\",  # image folder path (if bg_mode=\"image\" o \"mixed\")\n",
    "    \"bg_min_resize\":     256,         # min size for random resize of background images\n",
    "    \"bg_eval_color\":     [1.0, 1.0, 1.0],    # background used for final renders\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T20:54:08.525150Z",
     "iopub.status.busy": "2025-09-03T20:54:08.524867Z",
     "iopub.status.idle": "2025-09-03T20:54:11.985696Z",
     "shell.execute_reply": "2025-09-03T20:54:11.984983Z",
     "shell.execute_reply.started": "2025-09-03T20:54:08.525123Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import clip\n",
    "import copy\n",
    "import json\n",
    "import kaolin as kal\n",
    "import kaolin.ops.mesh\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import time\n",
    "\n",
    "from itertools import permutations, product\n",
    "from Normalization import MeshNormalizer\n",
    "from mesh import Mesh\n",
    "from pathlib import Path\n",
    "from render import Renderer\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import grad\n",
    "from torchvision import transforms\n",
    "from utils import device, color_mesh\n",
    "from utils import FourierFeatureTransform\n",
    "\n",
    "class NeuralHighlighter(nn.Module):\n",
    "    def __init__(self, depth, width, out_dim, input_dim=3, positional_encoding=False, sigma=5.0):\n",
    "        super(NeuralHighlighter, self).__init__()\n",
    "        layers = []\n",
    "        if positional_encoding:\n",
    "            layers.append(FourierFeatureTransform(input_dim, width, sigma))\n",
    "            layers.append(nn.Linear(width * 2 + input_dim, width))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.LayerNorm([width]))\n",
    "        else:\n",
    "            layers.append(nn.Linear(input_dim, width))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.LayerNorm([width]))\n",
    "        for i in range(depth):\n",
    "            layers.append(nn.Linear(width, width))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.LayerNorm([width]))\n",
    "        layers.append(nn.Linear(width, out_dim))\n",
    "        layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "        self.mlp = nn.ModuleList(layers)\n",
    "        print(self.mlp)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.mlp:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "def get_clip_model(clipmodel):\n",
    "    model, preprocess = clip.load(clipmodel, device=device, jit=False)\n",
    "    return model, preprocess\n",
    "\n",
    "# ================== HELPER FUNCTIONS =============================\n",
    "def save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background):\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        probs = mlp(vertices)\n",
    "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
    "        # for renders\n",
    "        one_hot = torch.zeros(probs.shape).to(device)\n",
    "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
    "        sampled_mesh = mesh\n",
    "\n",
    "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
    "        gray = torch.tensor([180, 180, 180]).to(device)\n",
    "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
    "        color_mesh(one_hot, sampled_mesh, colors)\n",
    "        eval_bg = torch.tensor(exp_config.get(\"bg_eval_color\", [1.,1.,1.]), device=device).float()\n",
    "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=5,\n",
    "                                                                        show=False,\n",
    "                                                                        center_azim=0,\n",
    "                                                                        center_elev=0,\n",
    "                                                                        std=4,\n",
    "                                                                        return_views=True,\n",
    "                                                                        lighting=True,\n",
    "                                                                        background=eval_bg)\n",
    "        # for mesh\n",
    "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
    "        final_color = torch.where(max_idx==0, highlight, gray)\n",
    "        mesh.export(os.path.join(log_dir, f\"{name}.ply\"), extension=\"ply\", color=final_color)\n",
    "        save_renders(log_dir, 0, rendered_images, name='final_render.jpg')\n",
    "def save_exp_config(config, output_dir):\n",
    "    import json, csv, os\n",
    "    # JSON\n",
    "    with open(os.path.join(output_dir, 'experiment_config.json'), 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    # CSV\n",
    "    csv_path = os.path.join(output_dir, 'experiments_summary.csv')\n",
    "    write_header = not os.path.exists(csv_path)\n",
    "    with open(csv_path, 'a', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=config.keys())\n",
    "        if write_header: writer.writeheader()\n",
    "        writer.writerow(config)\n",
    "\n",
    "def clip_loss(rendered_images: torch.Tensor,\n",
    "              text_embedding: torch.Tensor,\n",
    "              clip_model: nn.Module,\n",
    "              clip_transform: transforms.Compose,\n",
    "              augment_transform: transforms.Compose,\n",
    "              n_augs: int,\n",
    "              clipavg: str = \"view\") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Replichiamo esattamente la loss del codice ufficiale:\n",
    "    - n_augs==0: un solo forward con clip_transform\n",
    "    - n_augs>0: summation di n_augs forward con augment_transform\n",
    "    - clipavg=\"view\": media sulle viste prima di cosine‐similarity\n",
    "    - clipavg!=\"view\": media sulle coppie vista‐testo\n",
    "    \"\"\"\n",
    "    # step without augmentations\n",
    "    if n_augs == 0:\n",
    "        # 1)  resize+normalize\n",
    "        clip_imgs = clip_transform(rendered_images)            # (V,3,H,W)\n",
    "        # 2) encode CLIP\n",
    "        enc = clip_model.encode_image(clip_imgs)               # (V,D)\n",
    "        enc = enc / enc.norm(dim=1, keepdim=True)\n",
    "\n",
    "        # 3) test normalized\n",
    "        txt = text_embedding / text_embedding.norm(dim=1, keepdim=True)\n",
    "\n",
    "        # 4) compute loss\n",
    "        if clipavg == \"view\":\n",
    "            if txt.shape[0] > 1:\n",
    "                # view mean then cosine (similarity of means)\n",
    "                loss = -torch.cosine_similarity(enc.mean(0),\n",
    "                                                txt.mean(0), dim=0)\n",
    "            else:\n",
    "                loss = -torch.cosine_similarity(enc.mean(0, keepdim=True),\n",
    "                                                txt, dim=1)\n",
    "        else:\n",
    "            loss = -torch.mean(torch.cosine_similarity(enc, txt, dim=1))\n",
    "\n",
    "    # step with augmentations\n",
    "    else:\n",
    "        loss = 0.0\n",
    "        for _ in range(n_augs):\n",
    "            # 1) augment + normalize\n",
    "            aug = augment_transform(rendered_images)            # (V,3,H,W)\n",
    "            # 2) encode\n",
    "            enc_a = clip_model.encode_image(aug)\n",
    "            enc_a = enc_a / enc_a.norm(dim=1, keepdim=True)\n",
    "            # 3) text normalized\n",
    "            txt = text_embedding / text_embedding.norm(dim=1, keepdim=True)\n",
    "            # 4) compute loss\n",
    "            if clipavg == \"view\":\n",
    "                if txt.shape[0] > 1:\n",
    "                    loss -= torch.cosine_similarity(enc_a.mean(0),\n",
    "                                                    txt.mean(0), dim=0)\n",
    "                else:\n",
    "                    loss -= torch.cosine_similarity(enc_a.mean(0, keepdim=True),\n",
    "                                                    txt, dim=1)\n",
    "            else:\n",
    "                loss -= torch.mean(torch.cosine_similarity(enc_a, txt, dim=1))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "    \n",
    "def save_renders(dir, i, rendered_images, name=None):\n",
    "    if name is not None:\n",
    "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
    "    else:\n",
    "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T20:54:11.987396Z",
     "iopub.status.busy": "2025-09-03T20:54:11.987055Z",
     "iopub.status.idle": "2025-09-03T20:54:12.001592Z",
     "shell.execute_reply": "2025-09-03T20:54:12.000822Z",
     "shell.execute_reply.started": "2025-09-03T20:54:11.987376Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "import math\n",
    "import glob\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Background sampler + matting/compositing\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "def _to_4d(x):\n",
    "    # garantisce shape (B,3,H,W)\n",
    "    if x.dim() == 3:\n",
    "        x = x.unsqueeze(0)\n",
    "    return x\n",
    "\n",
    "def load_bg_pool(bg_dir):\n",
    "    paths = []\n",
    "    if os.path.isdir(bg_dir):\n",
    "        exts = (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.webp\")\n",
    "        for e in exts:\n",
    "            paths.extend(glob.glob(os.path.join(bg_dir, e)))\n",
    "    return paths\n",
    "\n",
    "_BG_POOL = load_bg_pool(exp_config[\"bg_dir\"])\n",
    "\n",
    "def _rand_solid(res):\n",
    "    c = torch.rand(3, device=device).view(3,1,1)\n",
    "    return c.expand(3, res, res)\n",
    "\n",
    "def _rand_noise(res):\n",
    "    return torch.rand(3, res, res, device=device)\n",
    "\n",
    "def _rand_image(res):\n",
    "    if len(_BG_POOL) == 0:\n",
    "        # fallback\n",
    "        return _rand_noise(res)\n",
    "    p = random.choice(_BG_POOL)\n",
    "    img = Image.open(p).convert(\"RGB\")\n",
    "    # ridimensiona lungo il lato minore\n",
    "    s = exp_config[\"bg_min_resize\"]\n",
    "    scale = s / min(img.size)\n",
    "    img = img.resize((int(img.size[0]*scale), int(img.size[1]*scale)))\n",
    "    # random crop a (res,res)\n",
    "    if img.size[0] == res and img.size[1] == res:\n",
    "        crop = img\n",
    "    else:\n",
    "        if img.size[0] < res or img.size[1] < res:\n",
    "            img = img.resize((max(res,img.size[0]), max(res,img.size[1])))\n",
    "        x0 = random.randint(0, img.size[0]-res)\n",
    "        y0 = random.randint(0, img.size[1]-res)\n",
    "        crop = img.crop((x0, y0, x0+res, y0+res))\n",
    "    bg = torch.from_numpy(np.array(crop)).float().permute(2,0,1) / 255.0\n",
    "    return bg.to(device)\n",
    "\n",
    "def sample_background_batch(batch, res, mode):\n",
    "    # mode: \"solid\" | \"noise\" | \"image\" | \"mixed\"\n",
    "    outs = []\n",
    "    for _ in range(batch):\n",
    "        m = mode\n",
    "        if mode == \"mixed\":\n",
    "            m = random.choice([\"solid\",\"noise\",\"image\"])\n",
    "        if m == \"solid\":\n",
    "            outs.append(_rand_solid(res))\n",
    "        elif m == \"noise\":\n",
    "            outs.append(_rand_noise(res))\n",
    "        else:\n",
    "            outs.append(_rand_image(res))\n",
    "    return torch.stack(outs, dim=0)  # (B,3,H,W)\n",
    "\n",
    "def composite_background(rendered, bg, key_rgb=(1.,1.,1.), tol=0.02, blur_px=0):\n",
    "    \"\"\"\n",
    "    rendered: (B,3,H,W), su colore uniforme key_rgb\n",
    "    bg:       (B,3,H,W)\n",
    "    easy Matting: change  the sane pixel of key color with the background.\n",
    "    \"\"\"\n",
    "    rendered = _to_4d(rendered)\n",
    "    B, C, H, W = rendered.shape\n",
    "\n",
    "    key = torch.tensor(key_rgb, device=rendered.device).view(1,3,1,1)\n",
    "    \n",
    "    dist = (rendered - key).abs().amax(dim=1, keepdim=True)  # (B,1,H,W)\n",
    "    mask_bg = (dist < tol).float()        # 1 = background; 0 = object\n",
    "\n",
    "    \n",
    "    if blur_px > 0:\n",
    "        import torch.nn.functional as F\n",
    "        \n",
    "        k = max(1, int(blur_px))\n",
    "        x = torch.arange(-k, k+1, device=rendered.device).float()\n",
    "        w = torch.exp(-(x**2)/(2*(k/2.0+1e-6)**2))\n",
    "        w = (w / w.sum()).view(1,1,-1,1)\n",
    "        mask_bg = F.conv2d(mask_bg, w, padding=(k,0), groups=mask_bg.size(1))\n",
    "        wT = w.permute(0,1,3,2)\n",
    "        mask_bg = F.conv2d(mask_bg, wT, padding=(0,k), groups=mask_bg.size(1))\n",
    "        mask_bg = mask_bg.clamp(0,1)\n",
    "\n",
    "    # ensure bg shape\n",
    "    if bg.dim() == 3: bg = bg.unsqueeze(0)\n",
    "    if bg.shape[-2:] != (H,W):\n",
    "        bg = torch.nn.functional.interpolate(bg, size=(H,W), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    comp = rendered*(1.0 - mask_bg) + bg*mask_bg\n",
    "    return comp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T20:54:12.002835Z",
     "iopub.status.busy": "2025-09-03T20:54:12.002563Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Constrain most sources of randomness\n",
    "# (some torch backwards functions within CLIP are non-determinstic)\n",
    "seed=exp_config[\"seed\"]\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "render_res = exp_config[\"render_res\"]\n",
    "learning_rate = exp_config[\"learning_rate\"]\n",
    "n_iter = exp_config[\"n_iter\"]\n",
    "res = exp_config[\"render_res\"]\n",
    "obj_path = exp_config[\"obj_path\"]\n",
    "n_augs = exp_config[\"n_augs\"]\n",
    "output_dir = exp_config[\"output_dir\"]\n",
    "clip_model = exp_config[\"clip_model_name\"]\n",
    "\n",
    "clip_model, preprocess = get_clip_model(clip_model)\n",
    "\n",
    "Path(os.path.join(output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "objbase, extension = os.path.splitext(os.path.basename(obj_path))\n",
    "\n",
    "render = Renderer(dim=(render_res, render_res))\n",
    "mesh = Mesh(obj_path)\n",
    "MeshNormalizer(mesh)()\n",
    "\n",
    "# Initialize variables\n",
    "background = torch.tensor((1., 1., 1.)).to(device)\n",
    "\n",
    "log_dir = output_dir\n",
    "\n",
    "# CLIP and Augmentation Transforms\n",
    "clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "clip_transform = transforms.Compose([\n",
    "    transforms.Resize((res, res), antialias=False),\n",
    "    clip_normalizer\n",
    "])\n",
    "augment_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(res, scale=(1, 1), antialias=False),\n",
    "    transforms.RandomPerspective(fill=1,\n",
    "                                 distortion_scale=exp_config[\"aug_params\"][\"distortion_scale\"],\n",
    "                                 p=exp_config[\"aug_params\"][\"p\"]),\n",
    "    clip_normalizer\n",
    "])\n",
    "\n",
    "# MLP Settings\n",
    "mlp = NeuralHighlighter(depth=exp_config[\"mlp_num_layers\"],\n",
    "    width=exp_config[\"mlp_hidden_dim\"],\n",
    "    out_dim=exp_config[\"mlp_out_dim\"],\n",
    "    input_dim=exp_config[\"mlp_input_dim\"],\n",
    "    positional_encoding=exp_config[\"positional_encoding\"],\n",
    "    sigma=exp_config[\"sigma\"]).to(device)\n",
    "optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n",
    "\n",
    "# list of possible colors\n",
    "rgb_to_color = {(204/255, 1., 0.): \"highlighter\", (180/255, 180/255, 180/255): \"gray\"}\n",
    "color_to_rgb = {\"highlighter\": [204/255, 1., 0.], \"gray\": [180/255, 180/255, 180/255]}\n",
    "full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]\n",
    "colors = torch.tensor(full_colors).to(device)\n",
    "\n",
    "\n",
    "# --- Prompt ---\n",
    "# encode prompt with CLIP\n",
    "prompt = exp_config[\"prompt\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    prompt_token = clip.tokenize([prompt]).to(device)\n",
    "    encoded_text = clip_model.encode_text(prompt_token)\n",
    "    encoded_text = encoded_text / encoded_text.norm(dim=1, keepdim=True)\n",
    "\n",
    "vertices = copy.deepcopy(mesh.vertices)\n",
    "n_views = exp_config[\"n_views\"]\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_iter = -1\n",
    "best_state = None\n",
    "\n",
    "losses = []\n",
    "start_time = time.time()\n",
    "# Optimization loop\n",
    "for i in tqdm(range(n_iter)):\n",
    "    optim.zero_grad()\n",
    "\n",
    "    # predict highlight probabilities\n",
    "    pred_class = mlp(vertices)\n",
    "\n",
    "    # color and render mesh\n",
    "    sampled_mesh = mesh\n",
    "    color_mesh(pred_class, sampled_mesh, colors)\n",
    "    \n",
    "    # --- Compositing background  ---\n",
    "    \n",
    "    background = torch.tensor(exp_config[\"bg_key_color\"], device=device).float()\n",
    "    rendered_images, elev, azim = render.render_views(sampled_mesh, num_views=n_views,\n",
    "                                                      show=False,\n",
    "                                                      center_azim=0,\n",
    "                                                      center_elev=0,\n",
    "                                                      std=4,\n",
    "                                                      return_views=True,\n",
    "                                                      lighting=True,\n",
    "                                                      background=background)\n",
    "    \n",
    "    \n",
    "    if exp_config[\"bg_mode\"] != \"none\" and random.random() < exp_config[\"bg_prob\"]:\n",
    "        B, C, H, W = rendered_images.shape\n",
    "        bgs = sample_background_batch(B, res=H, mode=exp_config[\"bg_mode\"])  # (B,3,H,W)\n",
    "        rendered_images = composite_background(\n",
    "            rendered_images, bgs,\n",
    "            key_rgb=exp_config[\"bg_key_color\"],\n",
    "            tol=exp_config[\"bg_key_tol\"],\n",
    "            blur_px=1\n",
    "        )\n",
    "\n",
    "\n",
    "    # Calculate CLIP Loss\n",
    "    loss = clip_loss(rendered_images,\n",
    "        encoded_text,\n",
    "        clip_model,\n",
    "        clip_transform,\n",
    "        augment_transform,\n",
    "        n_augs,\n",
    "        clipavg = exp_config[\"clipavg\"])\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optim.step()\n",
    "\n",
    "    # update variables + record loss\n",
    "    with torch.no_grad():\n",
    "        losses.append(loss.item())\n",
    "    # tracking of the best\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss  = loss.item()\n",
    "        best_iter  = i\n",
    "        best_state = copy.deepcopy(mlp.state_dict())\n",
    "        \n",
    "\n",
    "    # report results\n",
    "    if i % 100 == 0:\n",
    "        print(\"Last 100 CLIP score: {}\".format(np.mean(losses[-100:])))\n",
    "        save_renders(log_dir, i, rendered_images)\n",
    "        with open(os.path.join(log_dir, \"training_info.txt\"), \"a\") as f:\n",
    "            f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n",
    "\n",
    "# metrics\n",
    "final_loss       = losses[-1]\n",
    "exp_config[\"final_clip_score\"]       = -final_loss\n",
    "exp_config[\"avg_clip_score_last100\"] = -float(np.mean(losses[-100:]))\n",
    "exp_config[\"runtime_seconds\"]        = time.time() - start_time\n",
    "\n",
    "mlp.load_state_dict(best_state)\n",
    "exp_config[\"best_iter\"] = best_iter\n",
    "exp_config[\"best_clip_score\"] = -best_loss\n",
    "# save config + summary\n",
    "save_exp_config(exp_config, output_dir)\n",
    "# save results\n",
    "save_final_results(log_dir, f\"{objbase}_best_iter{best_iter}\", mesh, mlp, vertices, colors, render, background)\n",
    "\n",
    "# Save prompts\n",
    "with open(os.path.join(log_dir, \"prompt.txt\"), \"w\") as f:\n",
    "    f.write(prompt)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
