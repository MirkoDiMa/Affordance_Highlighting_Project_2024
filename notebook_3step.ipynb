{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-01T20:15:40.062073Z",
     "iopub.status.busy": "2025-09-01T20:15:40.061409Z",
     "iopub.status.idle": "2025-09-01T20:19:17.713175Z",
     "shell.execute_reply": "2025-09-01T20:19:17.712187Z",
     "shell.execute_reply.started": "2025-09-01T20:15:40.062049Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[33m  DEPRECATION: Building 'clip' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'clip'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[33mWARNING: Skipping kaolin as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytorch-lightning 2.5.2 requires torch>=2.1.0, but you have torch 2.0.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# GPU: CUDA 11.8, PyTorch 2.0.1 (Kaggle compatible)\n",
    "!pip install --upgrade -q pip\n",
    "!pip install -q git+https://github.com/openai/CLIP.git\n",
    "!pip uninstall -y -q kaolin\n",
    "\n",
    "\n",
    "!pip install -q torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 \\\n",
    "  -f https://download.pytorch.org/whl/cu118/torch_stable.html\n",
    "\n",
    "\n",
    "!pip install -q kaolin==0.17.0 \\\n",
    "  -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.0.1_cu118.html\n",
    "\n",
    "!pip install -q open3d==0.18.0 tqdm pillow gdown transforms3d scipy\n",
    "\n",
    "\n",
    "!rm -rf /kaggle/working/Affordance_Highlighting_Project_2024 /kaggle/working/output_PART3\n",
    "!git clone -q https://github.com/MirkoDiMa/Affordance_Highlighting_Project_2024.git /kaggle/working/Affordance_Highlighting_Project_2024\n",
    "\n",
    "import sys, os, numpy as np, random, json, time\n",
    "sys.path.append('/kaggle/working/Affordance_Highlighting_Project_2024')\n",
    "\n",
    "# CPU libs conservative \n",
    "os.environ[\"OMP_NUM_THREADS\"]=\"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"]=\"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"]=\"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-01T20:19:17.715193Z",
     "iopub.status.busy": "2025-09-01T20:19:17.714940Z",
     "iopub.status.idle": "2025-09-01T20:19:17.721361Z",
     "shell.execute_reply": "2025-09-01T20:19:17.720742Z",
     "shell.execute_reply.started": "2025-09-01T20:19:17.715166Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random, numpy as np\n",
    "\n",
    "REPO_ROOT   = '/kaggle/working/Affordance_Highlighting_Project_2024'\n",
    "OUTPUT_ROOT = '/kaggle/working/output_PART3'\n",
    "Path(OUTPUT_ROOT).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "exp_config = {\n",
    "    \"target_category\":   \"Bottle\",\n",
    "    \"target_affordance\": \"wrap_grasp\",\n",
    "    \"prompt_tpl\":        \"A 3D render of a gray {cat} with the grasped area highlighted\",\n",
    "\n",
    "    \"clip_model_name\": \"ViT-B/32\",\n",
    "\n",
    "    \"mlp_input_dim\":   3,\n",
    "    \"mlp_hidden_dim\":  256,\n",
    "    \"mlp_num_layers\":  6,\n",
    "    \"mlp_out_dim\":     2,\n",
    "    \"positional_encoding\": False,   \n",
    "    \"sigma\":           5.0,\n",
    "\n",
    "    \"render_res\":      224,\n",
    "    \"n_views\":         8,\n",
    "    \"learning_rate\":   1e-4,\n",
    "    \"n_iter_obj\":      2500,\n",
    "    \"n_augs\":          3,         \n",
    "    \"clipavg\":         \"view\",\n",
    "\n",
    "    \n",
    "    \"recon_mode\":      \"bare_poisson\",\n",
    "    \"poisson_depth\":   9,\n",
    "}\n",
    "\n",
    "seed = 45\n",
    "random.seed(seed); np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-01T20:19:17.722293Z",
     "iopub.status.busy": "2025-09-01T20:19:17.722055Z",
     "iopub.status.idle": "2025-09-01T20:19:27.928423Z",
     "shell.execute_reply": "2025-09-01T20:19:27.927713Z",
     "shell.execute_reply.started": "2025-09-01T20:19:17.722270Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Warp 1.8.1 initialized:\n",
      "   CUDA Toolkit 12.8, Driver 12.6\n",
      "   Devices:\n",
      "     \"cpu\"      : \"x86_64\"\n",
      "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
      "     \"cuda:1\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
      "   CUDA peer access:\n",
      "     Not supported\n",
      "   Kernel cache:\n",
      "     /root/.cache/warp/1.8.1\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn, torchvision\n",
    "import clip\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import open3d as o3d\n",
    "\n",
    "from render import Renderer\n",
    "from mesh import Mesh\n",
    "\n",
    "# Normalizer \n",
    "try:\n",
    "    from Normalization import MeshNormalizer\n",
    "except:\n",
    "    from MeshNormalizer import MeshNormalizer\n",
    "\n",
    "from utils import device, color_mesh\n",
    "try:\n",
    "    from utils import FourierFeatureTransform\n",
    "    HAS_FOURIER = True\n",
    "except:\n",
    "    HAS_FOURIER = False\n",
    "\n",
    "def set_seed(s):\n",
    "    torch.manual_seed(s); torch.cuda.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "    np.random.seed(s); random.seed(s)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class NeuralHighlighter(nn.Module):\n",
    "    \"\"\"MLP 2 classi (highlight / background) — identico stile ai compagni.\"\"\"\n",
    "    def __init__(self, depth, width, out_dim, input_dim=3, positional_encoding=False, sigma=5.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        if positional_encoding and HAS_FOURIER:\n",
    "            layers += [FourierFeatureTransform(input_dim, width, sigma),\n",
    "                       nn.Linear(width * 2 + input_dim, width)]\n",
    "        else:\n",
    "            layers += [nn.Linear(input_dim, width)]\n",
    "        layers += [nn.ReLU(), nn.LayerNorm([width])]\n",
    "        for _ in range(depth):\n",
    "            layers += [nn.Linear(width, width), nn.ReLU(), nn.LayerNorm([width])]\n",
    "        layers += [nn.Linear(width, out_dim), nn.Softmax(dim=1)]\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "    def forward(self, x): return self.mlp(x)\n",
    "\n",
    "def get_clip(name):\n",
    "    model, _ = clip.load(name, device=device, jit=False)\n",
    "    return model\n",
    "\n",
    "# CLIP transforms \n",
    "res = exp_config[\"render_res\"]\n",
    "clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                                       (0.26862954, 0.26130258, 0.27577711))\n",
    "clip_transform = transforms.Compose([transforms.Resize((res,res), antialias=False),\n",
    "                                     clip_normalizer])\n",
    "augment_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(res, scale=(0.95, 1.0), antialias=False),  # meno zoom\n",
    "    transforms.RandomPerspective(fill=1, p=0.5, distortion_scale=0.2),      # meno distorsione\n",
    "    clip_normalizer\n",
    "])\n",
    "\n",
    "\n",
    "def clip_loss(rendered_images, text_embedding, clip_model, n_augs, clipavg=\"view\"):\n",
    "    \"\"\"Stessa loss usata dai tuoi notebook precedenti e dai colleghi.\"\"\"\n",
    "    if n_augs == 0:\n",
    "        enc = clip_model.encode_image(clip_transform(rendered_images))\n",
    "        enc = enc / enc.norm(dim=1, keepdim=True)\n",
    "        txt = text_embedding / text_embedding.norm(dim=1, keepdim=True)\n",
    "        return -torch.cosine_similarity(enc.mean(0, keepdim=True), txt, dim=1)\n",
    "    loss = 0.0\n",
    "    for _ in range(n_augs):\n",
    "        enc = clip_model.encode_image(augment_transform(rendered_images))\n",
    "        enc = enc / enc.norm(dim=1, keepdim=True)\n",
    "        txt = text_embedding / text_embedding.norm(dim=1, keepdim=True)\n",
    "        loss -= torch.cosine_similarity(enc.mean(0, keepdim=True), txt, dim=1)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-01T20:19:27.929590Z",
     "iopub.status.busy": "2025-09-01T20:19:27.929170Z",
     "iopub.status.idle": "2025-09-01T20:19:50.847972Z",
     "shell.execute_reply": "2025-09-01T20:19:50.847155Z",
     "shell.execute_reply.started": "2025-09-01T20:19:27.929568Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?export=download&id=1siZtGusB1LfQVapTvNOiYi8aeKKAgcDF\n",
      "From (redirected): https://drive.google.com/uc?export=download&id=1siZtGusB1LfQVapTvNOiYi8aeKKAgcDF&confirm=t&uuid=085f6d7e-d9a7-4891-8d6c-3267631ae38d\n",
      "To: /kaggle/working/full-shape.zip\n",
      "100%|██████████| 558M/558M [00:02<00:00, 270MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train items: 16082 | Val items: 2285\n"
     ]
    }
   ],
   "source": [
    "import gdown, zipfile, os, pickle as pkl\n",
    "from os.path import join as opj\n",
    "\n",
    "DATASET_DIR = '/kaggle/working/dataset_affnet'\n",
    "os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "\n",
    "file_id = '1siZtGusB1LfQVapTvNOiYi8aeKKAgcDF'\n",
    "zip_path = '/kaggle/working/full-shape.zip'\n",
    "if not os.path.exists(zip_path):\n",
    "    gdown.download(f'https://drive.google.com/uc?export=download&id={file_id}', zip_path, quiet=False)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "    zf.extractall(DATASET_DIR)\n",
    "\n",
    "def load_split(split='train'):\n",
    "    with open(opj(DATASET_DIR, f'full_shape_{split}_data.pkl'), 'rb') as f:\n",
    "        data = pkl.load(f, encoding=\"latin1\")\n",
    "    return data\n",
    "\n",
    "train_data = load_split('train')\n",
    "val_data   = load_split('val')\n",
    "print(f\"Train items: {len(train_data)} | Val items: {len(val_data)}\")\n",
    "\n",
    "# Utility\n",
    "def select_indices_by_category(data, category, k=3):\n",
    "    idxs = [i for i, it in enumerate(data) if it[\"semantic class\"]==category]\n",
    "    random.shuffle(idxs)\n",
    "    return idxs[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-01T20:19:50.850096Z",
     "iopub.status.busy": "2025-09-01T20:19:50.849722Z",
     "iopub.status.idle": "2025-09-01T20:19:50.864538Z",
     "shell.execute_reply": "2025-09-01T20:19:50.863572Z",
     "shell.execute_reply.started": "2025-09-01T20:19:50.850079Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def reconstruct_mesh_from_points(points_xyz: np.ndarray,\n",
    "                                 out_obj: str,\n",
    "                                 mode=\"bpa_clean\",\n",
    "                                 poisson_depth=9,\n",
    "                                 fallback_bpa=True):\n",
    "    \"\"\"\n",
    "    mode=\"bpa_clean\": Ball Pivoting multiscala + cleanup + crop su AABB + smoothing\n",
    "    mode=\"bare_poisson\": Poisson depth=9\n",
    "    \"\"\"\n",
    "    # 1) Point cloud Open3D\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pts = points_xyz.astype(np.float64)\n",
    "    pcd.points = o3d.utility.Vector3dVector(pts)\n",
    "\n",
    "    pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamKNN(knn=16))\n",
    "    try:\n",
    "        pcd.orient_normals_consistent_tangent_plane(24)\n",
    "    except Exception:\n",
    "        pcd.orient_normals_towards_camera_location(np.array([0, 0, 3.0]))\n",
    "\n",
    "    if mode == \"bare_poisson\":\n",
    "        mesh, _ = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(\n",
    "            pcd, depth=int(poisson_depth)\n",
    "        )\n",
    "        aabb = pcd.get_axis_aligned_bounding_box()\n",
    "        bbox = aabb.scale(1.01, aabb.get_center())\n",
    "        mesh = mesh.crop(bbox)\n",
    "\n",
    "        # topological cleanup \n",
    "        mesh.remove_unreferenced_vertices()\n",
    "        mesh.remove_degenerate_triangles()\n",
    "        mesh.remove_duplicated_vertices()\n",
    "        mesh.remove_duplicated_triangles()\n",
    "        mesh.remove_non_manifold_edges()\n",
    "\n",
    "    elif mode == \"bpa_clean\":\n",
    "        # 2) BPA \n",
    "        dists = np.asarray(pcd.compute_nearest_neighbor_distance())\n",
    "        med = float(np.median(dists)) if dists.size else 0.01\n",
    "        radii = o3d.utility.DoubleVector([med*1.2, med*1.6, med*2.0, med*2.4, med*3.0, med*3.8])\n",
    "        mesh = o3d.geometry.TriangleMesh.create_from_point_cloud_ball_pivoting(pcd, radii)\n",
    "\n",
    "        # 3) Point cloud Open3D\n",
    "        mesh.remove_unreferenced_vertices()\n",
    "        mesh.remove_degenerate_triangles()\n",
    "        mesh.remove_duplicated_vertices()\n",
    "        mesh.remove_duplicated_triangles()\n",
    "        mesh.remove_non_manifold_edges()\n",
    "\n",
    "        # 4) Crop to AABB \n",
    "        aabb = pcd.get_axis_aligned_bounding_box()\n",
    "        bbox = aabb.scale(1.01, aabb.get_center())\n",
    "        mesh = mesh.crop(bbox)\n",
    "\n",
    "        # 5) Normali + smoothing\n",
    "        mesh.compute_vertex_normals()\n",
    "        if hasattr(mesh, \"filter_smooth_taubin\"):\n",
    "            mesh = mesh.filter_smooth_taubin(number_of_iterations=10)\n",
    "        else:\n",
    "            mesh = mesh.filter_smooth_simple(number_of_iterations=3)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown recon_mode: {mode}\")\n",
    "\n",
    "    # 6) Save OBJ \n",
    "    o3d.io.write_triangle_mesh(out_obj, mesh)\n",
    "    return mesh\n",
    "\n",
    "\n",
    "\n",
    "def map_vertices_to_points(verts: np.ndarray, points_xyz: np.ndarray):\n",
    "    tree = cKDTree(points_xyz)\n",
    "    _, idx = tree.query(verts, k=1)\n",
    "    return idx\n",
    "    \n",
    "def pc_normalize_np(pc: np.ndarray):\n",
    "    c = pc.mean(axis=0, keepdims=True)\n",
    "    pc0 = pc - c\n",
    "    r = np.linalg.norm(pc0, axis=1).max()\n",
    "    r = r if r > 0 else 1.0\n",
    "    return pc0 / r, c.squeeze(0), r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-01T20:19:50.865808Z",
     "iopub.status.busy": "2025-09-01T20:19:50.865497Z",
     "iopub.status.idle": "2025-09-01T20:19:58.930024Z",
     "shell.execute_reply": "2025-09-01T20:19:58.929444Z",
     "shell.execute_reply.started": "2025-09-01T20:19:50.865784Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 338M/338M [00:03<00:00, 112MiB/s]\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "clip_model = get_clip(exp_config[\"clip_model_name\"])\n",
    "\n",
    "palette = torch.tensor([[204/255, 1.0, 0.0],    \n",
    "                        [180/255, 180/255, 180/255]], device=device)\n",
    "\n",
    "def train_on_mesh(obj_path, prompt, out_dir,\n",
    "                  n_iter=2500, n_views=5, n_augs=5, lr=1e-4):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    renderer = Renderer(dim=(exp_config[\"render_res\"], exp_config[\"render_res\"]))\n",
    "    mesh     = Mesh(obj_path)\n",
    "    MeshNormalizer(mesh)()\n",
    "    vertices = mesh.vertices.clone()\n",
    "    background = torch.tensor((1.,1.,1.), device=device)\n",
    "\n",
    "    # Text CLIP\n",
    "    with torch.no_grad():\n",
    "        tok = clip.tokenize([prompt]).to(device)\n",
    "        txt = clip_model.encode_text(tok)\n",
    "        txt = txt / txt.norm(dim=1, keepdim=True)\n",
    "\n",
    "    mlp = NeuralHighlighter(\n",
    "        depth=exp_config[\"mlp_num_layers\"],\n",
    "        width=exp_config[\"mlp_hidden_dim\"],\n",
    "        out_dim=exp_config[\"mlp_out_dim\"],\n",
    "        input_dim=exp_config[\"mlp_input_dim\"],\n",
    "        positional_encoding=exp_config[\"positional_encoding\"],\n",
    "        sigma=exp_config[\"sigma\"]\n",
    "    ).to(device)\n",
    "\n",
    "    optim = torch.optim.Adam(mlp.parameters(), lr)\n",
    "    best_loss, best_state = float('inf'), None\n",
    "\n",
    "    for i in tqdm(range(n_iter), desc=f\"train {os.path.basename(obj_path)}\", leave=False):\n",
    "        optim.zero_grad()\n",
    "        pred = mlp(vertices)# (V,2)\n",
    "        p_high = pred[:,0]\n",
    "\n",
    "        area_target = 0.75  \n",
    "        area_prior = (p_high.mean() - area_target).pow(2) * 0.02 \n",
    "        \n",
    "        color_mesh(pred, mesh, palette)             \n",
    "        imgs, _, _ = renderer.render_views(\n",
    "            mesh, num_views=n_views, show=False,\n",
    "            center_azim=0, center_elev=0, std=4,\n",
    "            return_views=True, lighting=True,\n",
    "            background=background\n",
    "        )\n",
    "        \n",
    "        loss = clip_loss(imgs, txt, clip_model, n_augs, clipavg=exp_config[\"clipavg\"])\n",
    "        loss = loss.mean() + area_prior \n",
    "        (loss if torch.is_tensor(loss) else torch.tensor(loss, device=device)).mean().backward()\n",
    "        \n",
    "        optim.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val = float(loss.mean().item() if torch.is_tensor(loss) else loss)\n",
    "            if val < best_loss:\n",
    "                best_loss, best_state = val, {k:v.detach().clone() for k,v in mlp.state_dict().items()}\n",
    "\n",
    "        if i%100==0:\n",
    "            torchvision.utils.save_image(imgs, os.path.join(out_dir, f\"iter_{i}.jpg\"))\n",
    "\n",
    "    if best_state is not None:\n",
    "        mlp.load_state_dict(best_state)\n",
    "\n",
    "    # Renders + PLY \n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        probs   = mlp(vertices)\n",
    "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
    "        one_hot = torch.zeros_like(probs).scatter_(1, max_idx, 1)\n",
    "        color_mesh(one_hot, mesh, palette)\n",
    "        imgs, _, _ = renderer.render_views(\n",
    "            mesh, num_views=5, show=False, center_azim=0, center_elev=0,\n",
    "            std=4, return_views=True, lighting=True,\n",
    "            background=background\n",
    "        )\n",
    "        torchvision.utils.save_image(imgs, os.path.join(out_dir, f\"final_render.jpg\"))\n",
    "        final_color = torch.where(max_idx==0,\n",
    "                                  torch.tensor([204,255,0], device=device),\n",
    "                                  torch.tensor([180,180,180], device=device))\n",
    "        base = os.path.splitext(os.path.basename(obj_path))[0]\n",
    "        mesh.export(os.path.join(out_dir, f\"{base}.ply\"), extension=\"ply\", color=final_color)\n",
    "\n",
    "    return mlp\n",
    "\n",
    "def miou_from_mesh_preds(mlp, mesh_vertices, points_xyz, gt_binary):\n",
    "    \"\"\"\n",
    "    Assigns to each point in the cloud the label of the nearest mesh vertex.\n",
    "    This guarantees a 1:1 coverage between points and predictions and avoids 'holes' due to scattered voting.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from scipy.spatial import cKDTree\n",
    "\n",
    "    # 1) prediction per-vertex\n",
    "    with torch.no_grad():\n",
    "        logits = mlp(mesh_vertices).detach().cpu().numpy()   # (V,2)\n",
    "    vert_highlight = (logits.argmax(axis=1) == 0).astype(np.uint8)  # 0 = highlight\n",
    "\n",
    "    # 2) normalize the points as done by MeshNormalizer (center + unit scale)\n",
    "    def _pc_normalize_np(pc):\n",
    "        c = pc.mean(axis=0, keepdims=True)\n",
    "        pc0 = pc - c\n",
    "        r = np.linalg.norm(pc0, axis=1).max()\n",
    "        r = r if r > 0 else 1.0\n",
    "        return pc0 / r\n",
    "\n",
    "    pts_norm = _pc_normalize_np(points_xyz.astype(np.float32))\n",
    "    verts_np = mesh_vertices.detach().cpu().numpy()  \n",
    "\n",
    "    # 3) KDTree on  each vertex \n",
    "    tree = cKDTree(verts_np)\n",
    "    _, nn_vert = tree.query(pts_norm, k=1)  # (N,) \n",
    "\n",
    "    # 4) predicted mask per point\n",
    "    pred_mask = vert_highlight[nn_vert].astype(np.uint8)\n",
    "\n",
    "    # 5) IoU\n",
    "    gt_mask = (gt_binary > 0.5).astype(np.uint8)\n",
    "    inter = np.logical_and(pred_mask, gt_mask).sum()\n",
    "    union = np.logical_or (pred_mask, gt_mask).sum()\n",
    "    return (inter/union) if union > 0 else 0.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-01T20:27:48.831Z",
     "iopub.execute_input": "2025-09-01T20:19:58.930852Z",
     "iopub.status.busy": "2025-09-01T20:19:58.930669Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] 6e57c665a47d7d7730612f5c0ef21eb8  mIOU=0.2170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train train_Bottle_40e5d2c6e9e9cbbf5cafd3b1501bc74.obj:   2%|▏         | 58/2500 [00:10<07:32,  5.39it/s]"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, numpy as np, json\n",
    "\n",
    "target_cat = exp_config[\"target_category\"]\n",
    "target_aff = exp_config[\"target_affordance\"]\n",
    "prompt     = exp_config[\"prompt_tpl\"].format(cat=target_cat.lower(), aff=target_aff)\n",
    "\n",
    "out_mesh_dir = os.path.join(OUTPUT_ROOT, \"meshes\")\n",
    "out_res_dir  = os.path.join(OUTPUT_ROOT, \"results\")\n",
    "Path(out_mesh_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(out_res_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def get_points_and_gt(item, aff_name):\n",
    "    coords = item[\"full_shape\"][\"coordinate\"].astype(np.float32)     # (N,3)\n",
    "    labels = item[\"full_shape\"][\"label\"][aff_name].flatten()         # (N,)\n",
    "    return coords, labels\n",
    "\n",
    "def run_split(data, split_name, max_items=3):\n",
    "    idxs = select_indices_by_category(data, target_cat, k=max_items)\n",
    "    res = []\n",
    "    for idx in idxs:\n",
    "        item = data[idx]\n",
    "        pts, gt = get_points_and_gt(item, target_aff)\n",
    "        model_id = item[\"shape_id\"]\n",
    "\n",
    "        # 1) mesh reconstruction\n",
    "        obj_path = os.path.join(out_mesh_dir, f\"{split_name}_{target_cat}_{model_id}.obj\")\n",
    "        _mesh = reconstruct_mesh_from_points(\n",
    "            pts, obj_path,\n",
    "            mode=exp_config[\"recon_mode\"],\n",
    "            poisson_depth=exp_config[\"poisson_depth\"],\n",
    "            fallback_bpa=True  \n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        # 2) Training 3D-Highlighter \n",
    "        this_out = os.path.join(out_res_dir, f\"{split_name}_{target_cat}_{model_id}\")\n",
    "        mlp = train_on_mesh(\n",
    "            obj_path, prompt, this_out,\n",
    "            n_iter=exp_config[\"n_iter_obj\"],\n",
    "            n_views=exp_config[\"n_views\"],\n",
    "            n_augs=exp_config[\"n_augs\"],\n",
    "            lr=exp_config[\"learning_rate\"]\n",
    "        )\n",
    "\n",
    "        # 3) mIOU: mesh -> points\n",
    "        mesh_tmp = Mesh(obj_path)          \n",
    "        MeshNormalizer(mesh_tmp)()\n",
    "        miou = miou_from_mesh_preds(mlp, mesh_tmp.vertices, pts, gt)\n",
    "\n",
    "        res.append((model_id, float(miou)))\n",
    "        print(f\"[{split_name}] {model_id}  mIOU={miou:.4f}\")\n",
    "    return res\n",
    "\n",
    "set_seed(seed)\n",
    "train_results = run_split(train_data, \"train\", max_items=3)\n",
    "val_results   = run_split(val_data,   \"val\",   max_items=3)\n",
    "\n",
    "with open(os.path.join(OUTPUT_ROOT, 'optimization_results.json'), 'w') as f:\n",
    "    json.dump({\"train_results\": train_results, \"val_results\": val_results}, f, indent=2)\n",
    "\n",
    "print(\"TRAIN:\", train_results)\n",
    "print(\"VAL  :\", val_results)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
