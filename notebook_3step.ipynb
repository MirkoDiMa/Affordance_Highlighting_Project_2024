{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# GPU: CUDA 11.8, PyTorch 2.0.1 (Kaggle compatibile)\n!pip install --upgrade -q pip\n!pip install -q git+https://github.com/openai/CLIP.git\n!pip uninstall -y -q kaolin\n\n# Torch 2.0.1 + cu118 (match renderer del repo)\n!pip install -q torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 \\\n  -f https://download.pytorch.org/whl/cu118/torch_stable.html\n\n# Kaolin compatibile con Torch 2.0.1+cu118\n!pip install -q kaolin==0.17.0 \\\n  -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.0.1_cu118.html\n\n!pip install -q open3d==0.18.0 tqdm pillow gdown transforms3d scipy\n\n# Clona repo di base (renderer, Mesh, utils…)\n!rm -rf /kaggle/working/Affordance_Highlighting_Project_2024 /kaggle/working/output_PART3\n!git clone -q https://github.com/MirkoDiMa/Affordance_Highlighting_Project_2024.git /kaggle/working/Affordance_Highlighting_Project_2024\n\nimport sys, os, numpy as np, random, json, time\nsys.path.append('/kaggle/working/Affordance_Highlighting_Project_2024')\n\n# CPU libs conservative (stabilità su Kaggle)\nos.environ[\"OMP_NUM_THREADS\"]=\"1\"\nos.environ[\"OPENBLAS_NUM_THREADS\"]=\"1\"\nos.environ[\"MKL_NUM_THREADS\"]=\"1\"\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-01T20:15:40.061409Z","iopub.execute_input":"2025-09-01T20:15:40.062073Z","iopub.status.idle":"2025-09-01T20:19:17.713175Z","shell.execute_reply.started":"2025-09-01T20:15:40.062049Z","shell.execute_reply":"2025-09-01T20:19:17.712187Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[33m  DEPRECATION: Building 'clip' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'clip'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n\u001b[0m  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[33mWARNING: Skipping kaolin as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npytorch-lightning 2.5.2 requires torch>=2.1.0, but you have torch 2.0.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from pathlib import Path\nimport random, numpy as np\n\nREPO_ROOT   = '/kaggle/working/Affordance_Highlighting_Project_2024'\nOUTPUT_ROOT = '/kaggle/working/output_PART3'\nPath(OUTPUT_ROOT).mkdir(parents=True, exist_ok=True)\n\nexp_config = {\n    \"target_category\":   \"Bottle\",\n    \"target_affordance\": \"wrap_grasp\",\n    \"prompt_tpl\":        \"A 3D render of a gray {cat} with the grasped area highlighted\",\n\n    \"clip_model_name\": \"ViT-B/32\",\n\n    \"mlp_input_dim\":   3,\n    \"mlp_hidden_dim\":  256,\n    \"mlp_num_layers\":  6,\n    \"mlp_out_dim\":     2,\n    \"positional_encoding\": False,   # i compagni a volte True; puoi provarlo dopo\n    \"sigma\":           5.0,\n\n    \"render_res\":      224,\n    \"n_views\":         8,\n    \"learning_rate\":   1e-4,\n    \"n_iter_obj\":      2500,\n    \"n_augs\":          3,           # come nel loro esempio eval\n    \"clipavg\":         \"view\",\n\n    # Ricostruzione: POISSON “nudo” come pc_to_mesh.ipynb\n    \"recon_mode\":      \"bare_poisson\",\n    \"poisson_depth\":   9,\n}\n\nseed = 45\nrandom.seed(seed); np.random.seed(seed)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T20:19:17.714940Z","iopub.execute_input":"2025-09-01T20:19:17.715193Z","iopub.status.idle":"2025-09-01T20:19:17.721361Z","shell.execute_reply.started":"2025-09-01T20:19:17.715166Z","shell.execute_reply":"2025-09-01T20:19:17.720742Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch, torch.nn as nn, torchvision\nimport clip\nfrom torchvision import transforms\nfrom tqdm import tqdm\nimport open3d as o3d\n\nfrom render import Renderer\nfrom mesh import Mesh\n\n# Normalizer (nomi diversi nel repo)\ntry:\n    from Normalization import MeshNormalizer\nexcept:\n    from MeshNormalizer import MeshNormalizer\n\nfrom utils import device, color_mesh\ntry:\n    from utils import FourierFeatureTransform\n    HAS_FOURIER = True\nexcept:\n    HAS_FOURIER = False\n\ndef set_seed(s):\n    torch.manual_seed(s); torch.cuda.manual_seed(s); torch.cuda.manual_seed_all(s)\n    np.random.seed(s); random.seed(s)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\nclass NeuralHighlighter(nn.Module):\n    \"\"\"MLP 2 classi (highlight / background) — identico stile ai compagni.\"\"\"\n    def __init__(self, depth, width, out_dim, input_dim=3, positional_encoding=False, sigma=5.0):\n        super().__init__()\n        layers = []\n        if positional_encoding and HAS_FOURIER:\n            layers += [FourierFeatureTransform(input_dim, width, sigma),\n                       nn.Linear(width * 2 + input_dim, width)]\n        else:\n            layers += [nn.Linear(input_dim, width)]\n        layers += [nn.ReLU(), nn.LayerNorm([width])]\n        for _ in range(depth):\n            layers += [nn.Linear(width, width), nn.ReLU(), nn.LayerNorm([width])]\n        layers += [nn.Linear(width, out_dim), nn.Softmax(dim=1)]\n        self.mlp = nn.Sequential(*layers)\n    def forward(self, x): return self.mlp(x)\n\ndef get_clip(name):\n    model, _ = clip.load(name, device=device, jit=False)\n    return model\n\n# CLIP transforms (come nei tuoi step 1–2 e nei colleghi)\nres = exp_config[\"render_res\"]\nclip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n                                       (0.26862954, 0.26130258, 0.27577711))\nclip_transform = transforms.Compose([transforms.Resize((res,res), antialias=False),\n                                     clip_normalizer])\naugment_transform = transforms.Compose([\n    transforms.RandomResizedCrop(res, scale=(0.95, 1.0), antialias=False),  # meno zoom\n    transforms.RandomPerspective(fill=1, p=0.5, distortion_scale=0.2),      # meno distorsione\n    clip_normalizer\n])\n\n\ndef clip_loss(rendered_images, text_embedding, clip_model, n_augs, clipavg=\"view\"):\n    \"\"\"Stessa loss usata dai tuoi notebook precedenti e dai colleghi.\"\"\"\n    if n_augs == 0:\n        enc = clip_model.encode_image(clip_transform(rendered_images))\n        enc = enc / enc.norm(dim=1, keepdim=True)\n        txt = text_embedding / text_embedding.norm(dim=1, keepdim=True)\n        return -torch.cosine_similarity(enc.mean(0, keepdim=True), txt, dim=1)\n    loss = 0.0\n    for _ in range(n_augs):\n        enc = clip_model.encode_image(augment_transform(rendered_images))\n        enc = enc / enc.norm(dim=1, keepdim=True)\n        txt = text_embedding / text_embedding.norm(dim=1, keepdim=True)\n        loss -= torch.cosine_similarity(enc.mean(0, keepdim=True), txt, dim=1)\n    return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T20:19:17.722055Z","iopub.execute_input":"2025-09-01T20:19:17.722293Z","iopub.status.idle":"2025-09-01T20:19:27.928423Z","shell.execute_reply.started":"2025-09-01T20:19:17.722270Z","shell.execute_reply":"2025-09-01T20:19:27.927713Z"}},"outputs":[{"name":"stdout","text":"Jupyter environment detected. Enabling Open3D WebVisualizer.\n[Open3D INFO] WebRTC GUI backend enabled.\n[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\nWarp 1.8.1 initialized:\n   CUDA Toolkit 12.8, Driver 12.6\n   Devices:\n     \"cpu\"      : \"x86_64\"\n     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n     \"cuda:1\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n   CUDA peer access:\n     Not supported\n   Kernel cache:\n     /root/.cache/warp/1.8.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import gdown, zipfile, os, pickle as pkl\nfrom os.path import join as opj\n\nDATASET_DIR = '/kaggle/working/dataset_affnet'\nos.makedirs(DATASET_DIR, exist_ok=True)\n\n# full-shape.zip (stesso file id che stavi già usando)\nfile_id = '1siZtGusB1LfQVapTvNOiYi8aeKKAgcDF'\nzip_path = '/kaggle/working/full-shape.zip'\nif not os.path.exists(zip_path):\n    gdown.download(f'https://drive.google.com/uc?export=download&id={file_id}', zip_path, quiet=False)\n\nwith zipfile.ZipFile(zip_path, 'r') as zf:\n    zf.extractall(DATASET_DIR)\n\ndef load_split(split='train'):\n    with open(opj(DATASET_DIR, f'full_shape_{split}_data.pkl'), 'rb') as f:\n        data = pkl.load(f, encoding=\"latin1\")\n    return data\n\ntrain_data = load_split('train')\nval_data   = load_split('val')\nprint(f\"Train items: {len(train_data)} | Val items: {len(val_data)}\")\n\n# Utility: scegli indici di una certa categoria\ndef select_indices_by_category(data, category, k=3):\n    idxs = [i for i, it in enumerate(data) if it[\"semantic class\"]==category]\n    random.shuffle(idxs)\n    return idxs[:k]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T20:19:27.929170Z","iopub.execute_input":"2025-09-01T20:19:27.929590Z","iopub.status.idle":"2025-09-01T20:19:50.847972Z","shell.execute_reply.started":"2025-09-01T20:19:27.929568Z","shell.execute_reply":"2025-09-01T20:19:50.847155Z"}},"outputs":[{"name":"stderr","text":"Downloading...\nFrom (original): https://drive.google.com/uc?export=download&id=1siZtGusB1LfQVapTvNOiYi8aeKKAgcDF\nFrom (redirected): https://drive.google.com/uc?export=download&id=1siZtGusB1LfQVapTvNOiYi8aeKKAgcDF&confirm=t&uuid=085f6d7e-d9a7-4891-8d6c-3267631ae38d\nTo: /kaggle/working/full-shape.zip\n100%|██████████| 558M/558M [00:02<00:00, 270MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Train items: 16082 | Val items: 2285\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import open3d as o3d\nimport numpy as np\nfrom scipy.spatial import cKDTree\n\ndef reconstruct_mesh_from_points(points_xyz: np.ndarray,\n                                 out_obj: str,\n                                 mode=\"bpa_clean\",\n                                 poisson_depth=9,\n                                 fallback_bpa=True):\n    \"\"\"\n    mode=\"bpa_clean\": Ball Pivoting multiscala + cleanup + crop su AABB + smoothing\n                      (replica la pipeline del tuo notebook step-2)\n    mode=\"bare_poisson\": Poisson depth=9 minimale (solo per confronto)\n    \"\"\"\n    # 1) Point cloud Open3D\n    pcd = o3d.geometry.PointCloud()\n    pts = points_xyz.astype(np.float64)\n    pcd.points = o3d.utility.Vector3dVector(pts)\n\n    # Normali \"leggere\" robust-preprocessing (come step-2)\n    pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamKNN(knn=16))\n    try:\n        pcd.orient_normals_consistent_tangent_plane(24)\n    except Exception:\n        pcd.orient_normals_towards_camera_location(np.array([0, 0, 3.0]))\n\n    if mode == \"bare_poisson\":\n        mesh, _ = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(\n            pcd, depth=int(poisson_depth)\n        )\n        aabb = pcd.get_axis_aligned_bounding_box()\n        bbox = aabb.scale(1.01, aabb.get_center())\n        mesh = mesh.crop(bbox)\n\n        # Cleanup topologico\n        mesh.remove_unreferenced_vertices()\n        mesh.remove_degenerate_triangles()\n        mesh.remove_duplicated_vertices()\n        mesh.remove_duplicated_triangles()\n        mesh.remove_non_manifold_edges()\n\n    elif mode == \"bpa_clean\":\n        # 2) BPA multiscala con raggi derivati dalla distanza NN mediana (come step-2)\n        dists = np.asarray(pcd.compute_nearest_neighbor_distance())\n        med = float(np.median(dists)) if dists.size else 0.01\n        radii = o3d.utility.DoubleVector([med*1.2, med*1.6, med*2.0, med*2.4, med*3.0, med*3.8])\n        mesh = o3d.geometry.TriangleMesh.create_from_point_cloud_ball_pivoting(pcd, radii)\n\n        # 3) Cleanup topologico\n        mesh.remove_unreferenced_vertices()\n        mesh.remove_degenerate_triangles()\n        mesh.remove_duplicated_vertices()\n        mesh.remove_duplicated_triangles()\n        mesh.remove_non_manifold_edges()\n\n        # 4) Crop all’AABB del point cloud (leggermente espansa) per togliere gusci esterni\n        aabb = pcd.get_axis_aligned_bounding_box()\n        bbox = aabb.scale(1.01, aabb.get_center())\n        mesh = mesh.crop(bbox)\n\n        # 5) Normali + smoothing leggero\n        mesh.compute_vertex_normals()\n        if hasattr(mesh, \"filter_smooth_taubin\"):\n            mesh = mesh.filter_smooth_taubin(number_of_iterations=10)\n        else:\n            mesh = mesh.filter_smooth_simple(number_of_iterations=3)\n\n    else:\n        raise ValueError(f\"Unknown recon_mode: {mode}\")\n\n    # 6) Salva OBJ e ritorna\n    o3d.io.write_triangle_mesh(out_obj, mesh)\n    return mesh\n\n\n\ndef map_vertices_to_points(verts: np.ndarray, points_xyz: np.ndarray):\n    tree = cKDTree(points_xyz)\n    _, idx = tree.query(verts, k=1)\n    return idx\n    \ndef pc_normalize_np(pc: np.ndarray):\n    c = pc.mean(axis=0, keepdims=True)\n    pc0 = pc - c\n    r = np.linalg.norm(pc0, axis=1).max()\n    r = r if r > 0 else 1.0\n    return pc0 / r, c.squeeze(0), r\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T20:19:50.849722Z","iopub.execute_input":"2025-09-01T20:19:50.850096Z","iopub.status.idle":"2025-09-01T20:19:50.864538Z","shell.execute_reply.started":"2025-09-01T20:19:50.850079Z","shell.execute_reply":"2025-09-01T20:19:50.863572Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"set_seed(seed)\nclip_model = get_clip(exp_config[\"clip_model_name\"])\n\npalette = torch.tensor([[204/255, 1.0, 0.0],    # evidenziato (giallo/verde)\n                        [180/255, 180/255, 180/255]], device=device)\n\ndef train_on_mesh(obj_path, prompt, out_dir,\n                  n_iter=2500, n_views=5, n_augs=5, lr=1e-4):\n    os.makedirs(out_dir, exist_ok=True)\n\n    renderer = Renderer(dim=(exp_config[\"render_res\"], exp_config[\"render_res\"]))\n    mesh     = Mesh(obj_path)\n    MeshNormalizer(mesh)()  # normalizza mesh\n    vertices = mesh.vertices.clone()\n    background = torch.tensor((1.,1.,1.), device=device)\n\n    # Testo CLIP\n    with torch.no_grad():\n        tok = clip.tokenize([prompt]).to(device)\n        txt = clip_model.encode_text(tok)\n        txt = txt / txt.norm(dim=1, keepdim=True)\n\n    mlp = NeuralHighlighter(\n        depth=exp_config[\"mlp_num_layers\"],\n        width=exp_config[\"mlp_hidden_dim\"],\n        out_dim=exp_config[\"mlp_out_dim\"],\n        input_dim=exp_config[\"mlp_input_dim\"],\n        positional_encoding=exp_config[\"positional_encoding\"],\n        sigma=exp_config[\"sigma\"]\n    ).to(device)\n\n    optim = torch.optim.Adam(mlp.parameters(), lr)\n    best_loss, best_state = float('inf'), None\n\n    for i in tqdm(range(n_iter), desc=f\"train {os.path.basename(obj_path)}\", leave=False):\n        optim.zero_grad()\n        pred = mlp(vertices)# (V,2)\n        p_high = pred[:,0]\n\n        # --- MODIFICA QUESTE RIGHE ---\n        area_target = 0.75  # Un valore più realistico per un'impugnatura\n        area_prior = (p_high.mean() - area_target).pow(2) * 0.02 # Riduci anche il peso\n        \n        color_mesh(pred, mesh, palette)             # colora mesh\n        imgs, _, _ = renderer.render_views(\n            mesh, num_views=n_views, show=False,\n            center_azim=0, center_elev=0, std=4,\n            return_views=True, lighting=True,\n            background=background\n        )\n        \n        loss = clip_loss(imgs, txt, clip_model, n_augs, clipavg=exp_config[\"clipavg\"])\n        loss = loss.mean() + area_prior # Riaggiungi il prior alla loss\n        (loss if torch.is_tensor(loss) else torch.tensor(loss, device=device)).mean().backward()\n        \n        optim.step()\n\n        with torch.no_grad():\n            val = float(loss.mean().item() if torch.is_tensor(loss) else loss)\n            if val < best_loss:\n                best_loss, best_state = val, {k:v.detach().clone() for k,v in mlp.state_dict().items()}\n\n        if i%100==0:\n            torchvision.utils.save_image(imgs, os.path.join(out_dir, f\"iter_{i}.jpg\"))\n\n    if best_state is not None:\n        mlp.load_state_dict(best_state)\n\n    # Renders + PLY finale (come i colleghi)\n    mlp.eval()\n    with torch.no_grad():\n        probs   = mlp(vertices)\n        max_idx = torch.argmax(probs, 1, keepdim=True)\n        one_hot = torch.zeros_like(probs).scatter_(1, max_idx, 1)\n        color_mesh(one_hot, mesh, palette)\n        imgs, _, _ = renderer.render_views(\n            mesh, num_views=5, show=False, center_azim=0, center_elev=0,\n            std=4, return_views=True, lighting=True,\n            background=background\n        )\n        torchvision.utils.save_image(imgs, os.path.join(out_dir, f\"final_render.jpg\"))\n        final_color = torch.where(max_idx==0,\n                                  torch.tensor([204,255,0], device=device),\n                                  torch.tensor([180,180,180], device=device))\n        base = os.path.splitext(os.path.basename(obj_path))[0]\n        mesh.export(os.path.join(out_dir, f\"{base}.ply\"), extension=\"ply\", color=final_color)\n\n    return mlp\n\ndef miou_from_mesh_preds(mlp, mesh_vertices, points_xyz, gt_binary):\n    \"\"\"\n    Assegna ad ogni punto del cloud la label del vertice di mesh più vicino.\n    Questo garantisce copertura 1:1 punti→predizioni e evita i 'buchi' da voto sparso.\n    \"\"\"\n    import numpy as np\n    from scipy.spatial import cKDTree\n\n    # 1) predizione per-vertice\n    with torch.no_grad():\n        logits = mlp(mesh_vertices).detach().cpu().numpy()   # (V,2)\n    vert_highlight = (logits.argmax(axis=1) == 0).astype(np.uint8)  # 0 = highlight\n\n    # 2) normalizza i punti come fa MeshNormalizer (centro+scala unitaria)\n    def _pc_normalize_np(pc):\n        c = pc.mean(axis=0, keepdims=True)\n        pc0 = pc - c\n        r = np.linalg.norm(pc0, axis=1).max()\n        r = r if r > 0 else 1.0\n        return pc0 / r\n\n    pts_norm = _pc_normalize_np(points_xyz.astype(np.float32))\n    verts_np = mesh_vertices.detach().cpu().numpy()  # già normalizzati dalla MeshNormalizer\n\n    # 3) KDTree sui vertici e query su TUTTI i punti\n    tree = cKDTree(verts_np)\n    _, nn_vert = tree.query(pts_norm, k=1)  # (N,) indice del vertice più vicino\n\n    # 4) maschera predetta per punto\n    pred_mask = vert_highlight[nn_vert].astype(np.uint8)\n\n    # 5) IoU\n    gt_mask = (gt_binary > 0.5).astype(np.uint8)\n    inter = np.logical_and(pred_mask, gt_mask).sum()\n    union = np.logical_or (pred_mask, gt_mask).sum()\n    return (inter/union) if union > 0 else 0.0\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T20:19:50.865497Z","iopub.execute_input":"2025-09-01T20:19:50.865808Z","iopub.status.idle":"2025-09-01T20:19:58.930024Z","shell.execute_reply.started":"2025-09-01T20:19:50.865784Z","shell.execute_reply":"2025-09-01T20:19:58.929444Z"}},"outputs":[{"name":"stderr","text":"100%|████████████████████████████████████████| 338M/338M [00:03<00:00, 112MiB/s]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from pathlib import Path\nimport os, numpy as np, json\n\ntarget_cat = exp_config[\"target_category\"]\ntarget_aff = exp_config[\"target_affordance\"]\nprompt     = exp_config[\"prompt_tpl\"].format(cat=target_cat.lower(), aff=target_aff)\n\nout_mesh_dir = os.path.join(OUTPUT_ROOT, \"meshes\")\nout_res_dir  = os.path.join(OUTPUT_ROOT, \"results\")\nPath(out_mesh_dir).mkdir(parents=True, exist_ok=True)\nPath(out_res_dir).mkdir(parents=True, exist_ok=True)\n\ndef get_points_and_gt(item, aff_name):\n    coords = item[\"full_shape\"][\"coordinate\"].astype(np.float32)     # (N,3)\n    labels = item[\"full_shape\"][\"label\"][aff_name].flatten()         # (N,)\n    return coords, labels\n\ndef run_split(data, split_name, max_items=3):\n    idxs = select_indices_by_category(data, target_cat, k=max_items)\n    res = []\n    for idx in idxs:\n        item = data[idx]\n        pts, gt = get_points_and_gt(item, target_aff)\n        model_id = item[\"shape_id\"]\n\n        # 1) Ricostruzione mesh\n        obj_path = os.path.join(out_mesh_dir, f\"{split_name}_{target_cat}_{model_id}.obj\")\n        _mesh = reconstruct_mesh_from_points(\n            pts, obj_path,\n            mode=exp_config[\"recon_mode\"],\n            poisson_depth=exp_config[\"poisson_depth\"],\n            fallback_bpa=True   # ← questo va rimosso\n        )\n\n\n\n        # 2) Training 3D-Highlighter su mesh ricostruita\n        this_out = os.path.join(out_res_dir, f\"{split_name}_{target_cat}_{model_id}\")\n        mlp = train_on_mesh(\n            obj_path, prompt, this_out,\n            n_iter=exp_config[\"n_iter_obj\"],\n            n_views=exp_config[\"n_views\"],\n            n_augs=exp_config[\"n_augs\"],\n            lr=exp_config[\"learning_rate\"]\n        )\n\n        # 3) mIOU: proietta predizioni vertex→point e confronta con GT\n        mesh_tmp = Mesh(obj_path)          # ricarico per ottenere vertices normalizzati\n        MeshNormalizer(mesh_tmp)()\n        miou = miou_from_mesh_preds(mlp, mesh_tmp.vertices, pts, gt)\n\n        res.append((model_id, float(miou)))\n        print(f\"[{split_name}] {model_id}  mIOU={miou:.4f}\")\n    return res\n\nset_seed(seed)\ntrain_results = run_split(train_data, \"train\", max_items=3)\nval_results   = run_split(val_data,   \"val\",   max_items=3)\n\nwith open(os.path.join(OUTPUT_ROOT, 'optimization_results.json'), 'w') as f:\n    json.dump({\"train_results\": train_results, \"val_results\": val_results}, f, indent=2)\n\nprint(\"TRAIN:\", train_results)\nprint(\"VAL  :\", val_results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T20:19:58.930669Z","iopub.execute_input":"2025-09-01T20:19:58.930852Z","execution_failed":"2025-09-01T20:27:48.831Z"}},"outputs":[{"name":"stderr","text":"                                                                                                            \r","output_type":"stream"},{"name":"stdout","text":"[train] 6e57c665a47d7d7730612f5c0ef21eb8  mIOU=0.2170\n","output_type":"stream"},{"name":"stderr","text":"train train_Bottle_40e5d2c6e9e9cbbf5cafd3b1501bc74.obj:   2%|▏         | 58/2500 [00:10<07:32,  5.39it/s]","output_type":"stream"}],"execution_count":null}]}